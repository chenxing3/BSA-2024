{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biological Signals Analysis - Week 2 - Intro to Poisson Processes\n",
    "### Table of Contents:\n",
    "- [Part 1: The Binomial Distribution](#binomial)\n",
    "- [Part 2: The Poisson Distribution](#poisson)\n",
    "  - [Mathematical Note #1: a limit theorem](#math-note1)\n",
    "  - [Mathematical Note #2: ](#math-note2)\n",
    "  - [The masterstroke: a derivation from first principles](#derive)\n",
    "- [Part 3: Python Example](#python-example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Binomial Distribution <a id='binomial'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binomial distribution describes the probability of obtaining a certain number of successes in a series of independent trials, where each trial has only two possible outcomes: success or failure. \n",
    "\n",
    "Suppose we perform $ n $ independent trials of an experiment. In each trial:\n",
    "- The probability of success (denoted by $ S $) is $ p $.\n",
    "- The probability of failure (denoted by $ F $) is $ 1 - p $.\n",
    "\n",
    "Let $ X $ represent the total number of successes in $ n $ trials. We want to find $ E[X] $, the expected number of successes after $ n $ trials. The variable $ X $ can take any integer value from 0 to $ n $, representing the number of successes. We compute the expected value of $ X $ by definition, as the sum of all possible values of $ X $ weighted by their probabilities:\n",
    "\n",
    "$$\n",
    "E[X] = \\sum_{k=0}^{n} k \\cdot P(X = k).\n",
    "$$\n",
    "\n",
    "To proceed, we need to find $ P(X = k) $, the probability of obtaining exactly $ k $ successes in $ n $ trials. To do so, we count the number of ways to achieve exactly $ k $ successes in $ n $ trials and then multiply by the probability of any one of these outcomes occurring. Each specific arrangement of $ k $ successes and $ n - k $ failures corresponds to a sequence of successes and failures. For example, if $ n = 3 $ and $ k = 2 $, one possible sequence is $ SSF $ (two successes followed by one failure). The number of ways to arrange $ k $ successes in $ n $ trials is given by the binomial coefficient:\n",
    "\n",
    "$$\n",
    "\\binom{n}{k} = \\frac{n!}{k!(n - k)!}.\n",
    "$$\n",
    "\n",
    "Each specific sequence with $ k $ successes and $ n - k $ failures has a probability of occurring equal to $ p^k (1 - p)^{n - k} $. This is because each success has probability $ p $, and each failure has probability $ 1 - p $, and the trials are independent.The probability of obtaining exactly $ k $ successes in $ n $ trials is therefore:\n",
    "\n",
    "$$\n",
    "P(X = k) = \\binom{n}{k} p^k (1 - p)^{n - k}.\n",
    "$$\n",
    "\n",
    "Now, substituting $ P(X = k) $ into our expression for $ E[X] $, we get:\n",
    "\n",
    "$$\n",
    "E[X] = \\sum_{k=0}^{n} k \\cdot \\binom{n}{k} p^k (1 - p)^{n - k}.\n",
    "$$\n",
    "\n",
    "This sum is difficult to evaluate directly, so weâ€™ll take an alternative approach by rewriting $ X $ as the sum of $ n $ indicator variables representing the outcome of each trial. Let $ X_i $ be an indicator variable for the $ i $-th trial, where:\n",
    "\n",
    "$$\n",
    "X_i = \\begin{cases} \n",
    "      1 & \\text{if trial $ i $ is a success} \\\\\n",
    "      0 & \\text{if trial $ i $ is a failure}\n",
    "   \\end{cases}\n",
    "$$\n",
    "\n",
    "Then, we can write $ X $ as the sum of the $ X_i $ variables:\n",
    "\n",
    "$$\n",
    "X = X_1 + X_2 + \\dots + X_n.\n",
    "$$\n",
    "\n",
    "By the linearity of expectation, we can write:\n",
    "\n",
    "$$\n",
    "E[X] = E\\left(\\sum_{i=1}^{n} X_i\\right) = \\sum_{i=1}^{n} E[X_i].\n",
    "$$\n",
    "\n",
    "Now, each $ X_i $ is an indicator variable that equals 1 with probability $ p $ (success) and 0 with probability $ 1 - p $ (failure). The expected value of an indicator variable $ X_i $ is simply the probability that it equals 1:\n",
    "\n",
    "$$\n",
    "E[X_i] = p.\n",
    "$$\n",
    "\n",
    "Substituting $ E[X_i] = p $ into the sum, we get:\n",
    "$$\n",
    "E[X] = \\sum_{i=1}^{n} p = n \\cdot p.\n",
    "$$\n",
    "\n",
    "The expected value of a binomial random variable $ X \\sim \\text{Binomial}(n, p) $ is:\n",
    "\n",
    "$$\n",
    "E[X] = n \\cdot p.\n",
    "$$\n",
    "\n",
    "This result tells us that, on average, we can expect $ n \\cdot p $ successes in $ n $ trials when each trial has a success probability of $ p $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Poisson Distribution <a id='poisson'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose now that you are a neuroscientist trying to determine the expected number of times that a neuron fires in a given time period (for example, in a single hour). A logical first approach would be to treat the spikes of the neuron as a binomial random variable, take the expected value of this random variable (which we just derived), and break apart the hour into subsets such that each time subset is a single trial. For example, we may say that each minute in the hour is a single trial, and then calculate the expected number of spikes by obtaining the probability that a neuron spikes in a single minute and multiplying it by 60 minutes in an hour:\n",
    "\n",
    "$$\n",
    "E[\\text{number of spikes}] = 60 * p\n",
    "$$\n",
    "\n",
    "- Where $p$ is the probability that the given neuron exhibits a spike during a minute-long interval.\n",
    "\n",
    "This might serve well as an initial approximation. However, what if a neuron, on average, fires more than once per minute (a likely situation, given the firing dynamics of most neurons). Perhaps we should make our expression more granular. What if instead of defining a single trial as a single minute, we define a single trial as a single second? This would result in the following expression for calculating the expected number of spikes:\n",
    "\n",
    "$$\n",
    "E[\\text{number of spikes}] = 3600 * p\n",
    "$$\n",
    "\n",
    "- Where $p$ is the probability that the given neuron exhibits a spike during a second-long interval.\n",
    "\n",
    "This would very likely be a better approximation for the expected number of spikes the neuron will exhibit. However, will it be a perfect calculation? What if the neuron, on average, spikes more than once per second? In fact, what if the neuron spikes more than once per second even only sometimes? We may very well be tempted to answer: \"let's simply get more granular!\", and derive the expected number of spikes per millisecond interval, or even nanosecond interval. In fact, we could continue getting more granular ad infinetum. Perhaps, however, we could derive an expression for calculating the expected number of spikes as the interval of time defining each trial approaches an infinitely small value? (spoiler: this is exactly what we will do).\n",
    "\n",
    "In fact, if you do this, you will end up getting the formula for the *poisson distribution*. And this is interesting, because often times you will simply be presented with the formula for the poisson distribution without being shown where it comes from, and you can simply plug in the numbers and use it. But it is truly neat to know that the poisson distribution is simply the binomial distribution taken to its limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, before we commence with our derivation of the Poisson distribution, we will briefly review two mathematical tools that we will use in our derivation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math Note #1 : a limit theorem <a id='math-note1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following limit can be evaluated as follows:\n",
    "\n",
    "$$\n",
    "\\lim_{x \\to \\inf} (1+ \\frac{a}{x})^x = e^a\n",
    "$$\n",
    "\n",
    "Proof:\n",
    "\n",
    "Let $\\frac{1}{n} = \\frac{a}{x}$, thus: $x = n^a$\n",
    "\n",
    "As $x$ approaches infinity, what does $a$ approach? --> also infinity. Thus, if we substitude in the terms:\n",
    "\n",
    "$$\n",
    "\\lim_{n \\to \\inf} (1+ \\frac{1}{n})^na\n",
    "$$\n",
    "\n",
    "Which is the same as:\n",
    "\n",
    "$$\n",
    "\\lim_{n \\to \\inf} ((1+ \\frac{1}{n})^n)^a\n",
    "$$\n",
    "\n",
    "Which is the same as:\n",
    "\n",
    "$$\n",
    "(\\lim_{n \\to \\inf} (1+ \\frac{1}{n})^n)^a\n",
    "$$\n",
    "\n",
    "And it turns out that one of the ways to calculate $e$ is this expression:\n",
    "\n",
    "$$\n",
    "\\lim_{n \\to \\inf} (1+ \\frac{1}{n})^n\n",
    "$$\n",
    "\n",
    "You can try it out on your calculator to confirm, by plugging in bigger and bigger values for $n$. Thus, the final expression evaluates to:\n",
    "\n",
    "$$\n",
    "e^a\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math Note #2: a factorial theorem <a id='math-note2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following expression can be evaluated as follows:\n",
    "\n",
    "$$\n",
    "\\frac{x!}{(x-k)!} = (x)(x-1)(x-2)\\cdots(x-k+1)\n",
    "$$\n",
    "\n",
    "And there are exactly $k$ terms in the right side of the expression.\n",
    "\n",
    "For example:\n",
    "\n",
    "$$\n",
    "\\frac{7!}{(7-2)!} = \\frac{(7)(6)(5)(4)(3)(2)(1)}{(5)(4)(3)(2)(1)} = (7)(6) = (7)(7-2+1)\n",
    "$$\n",
    "\n",
    "Note that there are $2$ ($k$) terms in the final expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Masterstroke: a derivation from first principles <a id='derive'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the tools and background we need to derive the Poisson Distribution. To remind ourselves, we are a neuroscientist studying a spiking neuron (a random variable). We have observed this neuron for hours on end, and we found a pretty decent expected value for the number of spikes the neuron will exhibit in an hour, which we define as $\\lambda$: $E(\\text{spikes}) = \\lambda$. We stated that we want to model this expected value as a binomial distribution, and so we define: \n",
    "\n",
    "\n",
    "$$\n",
    "E(\\text{spikes}) = \\lambda = n*p\n",
    "$$\n",
    "\n",
    "Where $n$ is a single trial. And to remind ourselves again, we could view a trial as some subset of the total amount of time we are measuring spikes in. So $\\lambda$ would be the total number of spikes in an hour, and $n$ would be some subset of an hour. We then wanted to take the limit as $n$ approaches infinity, to get the most accurate estimate of the number of spikes, and avoid mistakes that would arise from a neuron that spikes more than once in any non-infinitesimally-small period of time. To evaluate the limit as $n \\rightarrow \\inf$, we begin as follows: \n",
    "\n",
    "$$\n",
    "P(X=k) = \\lim_{n \\to \\inf} {n \\choose k} (\\text{probability of spike})\n",
    "$$\n",
    "\n",
    "In simple terms, the probability that the neuron $X$ will spike $k$ times in a given interval is given by the limit as $n \\rightarrow \\inf$ of n-choose-k spikes (because we have ifinite $n$ moments during which the neuron can spike, and we wish to observe $k$ successful moments during which the neuron spikes), times the probability of success (the probability that a neuron spikes in any given moment). Since $E(\\text{spikes}) = \\lambda = n*p$, the probability of success is thus: $p_{\\text{success}} = \\frac{\\lambda}{n}. Since we are interested in the probability of $k$ successes, we must raise this probability to the $k$-th power. We also have to multiply the expression by the probability of failure, which is simply $p_{\\text{failure}} = 1- \\frac{\\lambda}{n}$. Once again, since we are interested in observing $k$ successes, we raise this expression to $n-k$ failures (the number of times out of $n$ where failure is to be observed). Thus, the above expression evaluates to:\n",
    "\n",
    "$$\n",
    "P(X=k) = \\lim_{n \\to \\inf} {n \\choose k} (\\frac{\\lambda}{n})^k (1- \\frac{\\lambda}{n})^{n-k}\n",
    "$$\n",
    "\n",
    "Evaluating this expression further yields:\n",
    "\n",
    "$$\n",
    "P(X=k) = \\lim_{n \\to \\inf} \\frac{n!}{(n-k)!k!} (\\frac{\\lambda^k}{n^k}) (1- \\frac{\\lambda}{n})^{n} (1- \\frac{\\lambda}{n})^{-k} \n",
    "$$\n",
    "\n",
    "If we use our factorial theorem tool, the expression evaluates to:\n",
    "\n",
    "$$\n",
    "P(X=k) = \\lim_{n \\to \\inf} \\frac{(n)(n-1)(n-2)\\cdots(n-k+1)}{k!} (\\frac{\\lambda^k}{n^k}) (1- \\frac{\\lambda}{n})^{n} (1- \\frac{\\lambda}{n})^{-k} \n",
    "$$\n",
    "\n",
    "Where there are k terms in the numerator of the first term.\n",
    "If we swap the denominators of the first two terms (which we are allowed to do due to multiplication rules), the expression evaluates to: \n",
    "\n",
    "$$\n",
    "P(X=k) = \\lim_{n \\to \\inf} \\frac{(n)(n-1)(n-2)\\cdots(n-k+1)}{n^k} (\\frac{\\lambda^k}{k!}) (1- \\frac{\\lambda}{n})^{n} (1- \\frac{\\lambda}{n})^{-k} \n",
    "$$\n",
    "\n",
    "Since the $\\lim_{x \\to a} f(x) g(x) = \\lim_{x \\to a} f(x) \\lim_{x \\to a} g(x)$, we can break apart the expression to:\n",
    "\n",
    "$$\n",
    "P(X=k) = \\lim_{n \\to \\inf} \\frac{(n)(n-1)(n-2)\\cdots(n-k+1)}{n^k} \\lim_{n \\to \\inf}(\\frac{\\lambda^k}{k!}) \\lim_{n \\to \\inf}(1- \\frac{\\lambda}{n})^{n} \\lim_{n \\to \\inf}(1- \\frac{\\lambda}{n})^{-k} \n",
    "$$\n",
    "\n",
    "In the numerator of our first limit, since we have $k$ terms being multiplied, if we factor out the expression, the largest degree term in the numerator will be $n^k$, since during the factoring we will be multiplying $n$ by itself $k$ times. Thus, the first limit can be simplified to:\n",
    "\n",
    "$$\n",
    "\\lim_{n \\to \\inf} \\frac{n^k + \\cdots}{n^k}(\\frac{\\lambda^k}{k!})\n",
    "$$\n",
    "\n",
    "And the remaining terms in the numerator don't really interest us, because $n^k$ is the largest degree term, and we are evaluating the limit $\\lim_{n \\to \\inf}$. Since we have $n^k$ in both the numerator and in the denominator, this limit of this term would thus be 1, since these are the highest degree terms. Thus, we have already simplified our expression to:\n",
    "\n",
    "$$\n",
    "P(X=k) = 1*(\\frac{\\lambda^k}{k!}) \\lim_{n \\to \\inf}(1- \\frac{\\lambda}{n})^{n} \\lim_{n \\to \\inf}(1- \\frac{\\lambda}{n})^{-k} \n",
    "$$\n",
    "\n",
    "Using the theorem we went over, which showed that $\\lim_{x \\to \\inf} (1+ \\frac{a}{x})^x = e^a $, we can evaluate the next term in the expression $\\lim_{n \\to \\inf}(1- \\frac{\\lambda}{n})^{n} = e^{-\\lambda} $, which means that our expression evaluates to:\n",
    "\n",
    "$$\n",
    "P(X=k) = 1*(\\frac{\\lambda^k}{k!}) * e^{-\\lambda} * \\lim_{n \\to \\inf}(1- \\frac{\\lambda}{n})^{-k} \n",
    "$$\n",
    "\n",
    "What about the last term in the expression? Well, since $\\lambda$ is a constant, and is being divided by $n$, which is going to infinity, the term $\\frac{\\lambda}{n} will go to zero, which will mean that term evaluates to $(1-0)^{-k}$. Since $1$ to any power is always $1$, the final term in the expression also becomes $1$, yielding the following: \n",
    "\n",
    "$$\n",
    "P(X=k) = 1*(\\frac{\\lambda^k}{k!}) * e^{-\\lambda} * 1 \n",
    "$$\n",
    "\n",
    "Or, put more simply, the probability of observing $k$ spikes in a given time period can be expressed as:\n",
    "\n",
    "$$\n",
    "P(X=k) = \\lim_{n \\to \\inf} {n \\choose k} (\\frac{\\lambda}{n})^k (1- \\frac{\\lambda}{n})^{n-k} = (\\frac{\\lambda^k}{k!}) * e^{-\\lambda} \n",
    "$$\n",
    "\n",
    "And, you may remember from our first exercise, that the poisson distribution is expressed as: $P(X=k) = (\\frac{\\lambda^k}{k!}) * e^{-\\lambda}$. And so, we have arrived at this definition of the occurrence of a Poisson events by deriving it entirely from first principles. This is super neat, because when you see this expression without any context, you may not in any way guess that it is related to the binomial theorem. But now you have in your mind not only the understanding of how it relates to the binomial theorem, but also how to derive both distributions from first principles. \n",
    "\n",
    "*So why did we do all this?* Oftentimes in biological sciences, we may be tempted to simply use mathematical tools without fully understanding them. This may be quick and efficient, and most of the time may even be sufficient to produce good scientific results without any mistakes. However, it is far more useful and far more beautiful to understand the tools you are using. That way, when you look at the results they output, you will have a much deeper intuition for what those results actually mean. Additionally, you will be more likely to use the tools correctly, and spot mistakes when they are used incorrectly. Finally, the joy of understanding these beautiful expressions is a worthy goal inof itself, and will add much color and happiness to your life as an academic, scientist, and technologist."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
